{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfeAz6elDcns"
   },
   "source": [
    "<div style=\"clear:left\"></div>\n",
    "\n",
    "# FBA Tutorial 8b - Plotting\n",
    "\n",
    "As in the previous tutorial, we will be using the **admissions.csv** file (available from the moodle page). This file describes people who are applying for a postgraduate degree at a US university, and indicates 3 relevant features about an applicant… as well as whether they got into to the masters or PhD course they were applying for when they were selected by hand (note that if we were doing a full analytics project, this would be our target class!):\n",
    "\n",
    "* The dataset has a binary output feature (i.e. dependent variable) called \"admit\". \n",
    "* There are three predictor variables: gre, gpa and ranking. \n",
    "* Variables gre (a exam result score) and gpa (the person’s grade point average) are continuous.\n",
    "* The variable ranking takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k67aw6biDcoP"
   },
   "source": [
    "# Section A\n",
    "\n",
    "This section focus on data loading and pre-processing using Pandas\n",
    "\n",
    "### Step A1 \n",
    "Using the **pandas** library and its **read_csv()** function, load the data of the file **admissions.csv** into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O30HSoudDcoQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSTFuQ23DcoT"
   },
   "source": [
    "### Step A2\n",
    "\n",
    "This should become a good habit, each time you load new data, check that it has been loaded correctly using the **head()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gx1KHJatDcoV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tn81AZ69Dcof"
   },
   "source": [
    "### Step A3 - Updating a column\n",
    "\n",
    "For clarity we want to replace the values of the column admit to True if it is equal to 1 or False if it is equal to 0. For this, you will need to use the **loc( )** method from **Pandas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YZ_WTwpDcog"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0Sk0xHYDcok",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It is always good to check your operation\n",
    "# had the expected output!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXL8EtTUDcoq"
   },
   "source": [
    "### Step A4 - An overview of our data\n",
    "\n",
    "So far so good, but one final thing - we really want to show how each feature’s values differ for one class compared to the other  (this would be useful for example, in a coursework to investigate who buy a product and don’t)\n",
    "\n",
    "Let’s do this now by separating the positives (admitted - True) from the negatives (rejected - False) in our dataset. Using pandas filtering, assign to a variable **pos** the dataframe of students who were admitted and to a variable **neg** the students that were rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ2yXaozUS_6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2QvGahWDcos"
   },
   "source": [
    "What this means is that we now can print summary statistics to compare between successful and unsuccessful applicants using the **describe( )** method on the two separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jPrc9OFDcos"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4HEq9r3UTAD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2T-4Uf3VUTAI"
   },
   "source": [
    "What, if anything, can you see that is surprising or noteworthy to report back concerning these? (tip - check which feature is higher for admissions and rejections)\n",
    "\n",
    "***What would you comment on concerning these comparisons between rejections and admissions to feedback to a client?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfV-Nzl7UTAJ"
   },
   "source": [
    "# Section B\n",
    "\n",
    "This section focuses on the use of matplotlib.\n",
    "\n",
    "### Step B1 - A picture is as good as a thousand words...\n",
    "\n",
    "There are definitely some oddities here. And how best to illustrate those? Well, statistics are all good, but graphs are often better to demonstrate things you have found in your analyses. As you’ve seen in another (rival) module, Tableau is a really useful bit of software for throwing graphs together - its always fine to use good software like that.\n",
    "\n",
    "But it’s also very easy in python thanks to the “matplotlib”  library (and its extensions) so let’s just see how we could make a bar chart comparing the number of datapoints which were positive and negative.\n",
    "\n",
    "First, here is how we can do it the 'manual' way using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ye0LkkN5UTAL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['rejected', 'admitted']\n",
    "values = [len(neg), len(pos)]\n",
    "plt.bar([0, 1], values, align='center', color=['red', 'green'])\n",
    "plt.xticks([0, 1], labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7t9uPsbLUTAQ"
   },
   "source": [
    "Well, that wasn’t too bad… but it was several lines let’s break what is going on here down line by line:\n",
    "1. We import the matplotlib library (and call it “plt”) so we can use its functions\n",
    "2. We create a list of the labels for the bars.\n",
    "3. We create a list of values for the bar’s heights (in this case the no. of items in each category).\n",
    "4. We create the bar chart (which a column at positions 0 and 1, aligning the bars to the centre and making them red and green in colour.\n",
    "5. We add the bar’s labels to the plot.\n",
    "6. And finally we show the results on screen.\n",
    "\n",
    "### Step B2 - Visualising the mean\n",
    "\n",
    "Now let’s create a bar chart for the mean gpa score for each class by copying and altering the code of the previous step.\n",
    "\n",
    "You can compute the mean using either a Pandas method or a Numpy function for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wd8c07ZwUTAR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0E7-hQGHUTAV"
   },
   "source": [
    "### Step B3 - Visualising the mean variance\n",
    "\n",
    "Create a bar chart for the mean of the variance score for each class (you might want to use numpy function np.var() to do this rather than square the standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1UGyOzeUTAW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeFrWuTgUTAb"
   },
   "source": [
    "Now, yes, these images still have a horrible grey background, and the fonts aren’t the most beautiful, but these are all editable. Take a look at the tutorials online for making beautiful matplotlib graphs (they are easy to cut and paste) if you get a chance. \n",
    "\n",
    "However, we have a second option using this time the **seaborn** library.\n",
    "\n",
    "# Section C\n",
    "\n",
    "This section focuses on the use of Seaborn\n",
    "\n",
    "### Step C1 - Repoducing B1 in Seaborn\n",
    "\n",
    "We want to reproduce the graph done at Step B1 using Seaborn this time. The first thing, as usual and as described in the lecture, is to import the necessary module(s).\n",
    "\n",
    "Note that in this case, matplotlib.pyplot has already been imported for the previous questions so we don't need to re-import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utYPjaevUTAc"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpgaPRiWUTAg"
   },
   "source": [
    "We can then produce the bar plot using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YVzJbcWUTAi"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='admit', y='gpa', data=df)\n",
    "\n",
    "# OR\n",
    "#sns.barplot(data=[neg.gpa, pos.gpa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFfwN2ORUTAp"
   },
   "source": [
    "### Step C2 - Styling the plot\n",
    "\n",
    "Seaborn has a range of colour palettes (look them up on google). You can specify a palette by adding the following argument to your plotting code (in our case the call to the barplot function):\n",
    "\n",
    "> palette = 'Set1'\n",
    "\n",
    "But it also has lots of built in style’s for the whole graph. Let’s try one now by adding the following line before your barplot command:\n",
    "\n",
    "> sns.set_style('whitegrid')\n",
    "\n",
    "Rerun and you’ll see you now have a graph worth of using in a report. Feel free to take a look at some other options for these styles:\n",
    "* darkgrid\n",
    "* whitegrid\n",
    "* dark\n",
    "* white\n",
    "* ticks\n",
    "\n",
    "Try different styles to make the plot a bit prettier !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkDkE0_HUTAr"
   },
   "source": [
    "### Step C3 - Boxplots\n",
    "\n",
    "Finally, let’s abandon bar charts all together. They are fine for just listing the counts of admissions here, but they will tell us very little information about our covariates - gpa scores, gre scores and university rankings. What we really want as analysts are box plots!\n",
    "\n",
    "Copy the code from C1 and convert it to use boxplots instead. You can refer to the documentation to find how the boxplot function is used:\n",
    "\n",
    "http://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veBQQtKqUTAt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd5QJqHDUTAy"
   },
   "source": [
    "Each solid box represents the second and third quartiles of the data indicating the spread/variance of the data, the horizontal line through each box is the mean, and the vertical lines show the limits of the dataset. All useful stuff for interpretation.\n",
    "\n",
    "That’s all there is to it for producing nice graphs. There’s a huge amount of options to play around with, and I recommend having a go on the different options for adding labels, changing fonts, using different palettes and so on (I’ll leave you to google the endless tutorials and documentation on matplotlib and seaborn at your leisure!)\n",
    "\n",
    "*Having said that there is also a competitor to boxplots that’s worth noting in the form of violinplots*\n",
    "\n",
    "### Step C4 - Violinplots\n",
    "\n",
    "Finally produce a violin plot for the GPA scores using the sns.violinplot() method. It’s as simple as changing the method name in your code. Once again, you can use the online documentation to find the different options available is you need further tweaking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-OrphyJUTAz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H8YhBpRUTA3"
   },
   "source": [
    "Now we can see the exact distribution of the data. Not bad, but is it better? You decide.\n",
    "\n",
    "## Section D\n",
    "\n",
    "In this section we are going to focus on the relationship between GRE and GPA scores using different techniques and adding sklearn to our set of tools.\n",
    "\n",
    "### Step D1 - Pearson Correlation\n",
    "\n",
    "Ok let’s start exploring the data with a model. Pearson Correlation is the key factor in a linear regression - the further it is from zero the stronger the linear relationship (even though we all know now, that linear relationships certainly don’t tell the whole story). Let’s use numpy to find that correlation between the GRE and GPA scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzwBrlFMDcpK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = np.corrcoef(df.gpa, df.gre)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHfVAH5RDcpP"
   },
   "source": [
    "As you can see, you get a matrix output (you can check the type using **print(type(r))** if you want). The only score that matters is the first line, second column (or second line, first column which is the same)  - that’s the correlation between the variables we care about.. Given that this number is part of a matrix (ndarray), try to extract it using **Numpy** indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ig1YzvpODcpQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAz6KlawDcpU"
   },
   "source": [
    "More than numbers, data analysts need to understand what they mean. You have now obtained a number for the Pearson Correlation of our two variables. What do you think of this correlation? Is this correlation strong, low or medium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiTU90HvDcpV"
   },
   "source": [
    "### Step D2 - Importing Sklearn\n",
    "\n",
    "This is still just a descriptive correlation score and won’t actually help us predict anything. To actually try and predict someone’s GRE exam score from their GPA average we need a linear model. You have seen that it can easily be done using **Orange** with a graphical interface. It is almost as easy in Python thanks to the **sklearn** library, so let's give it a go!\n",
    "\n",
    "The first step is to import the library using the **from** *module* **import** *submodule* syntax. From the library **sklearn** we want to import the submodule **linear_model**, try to write that import statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Zwbuq6QDcpW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1X-wUSX9DcpZ"
   },
   "source": [
    "### Step D3 - Create our features\n",
    "\n",
    "It’s a tradition from the olden days of yore, when people still used punchcards to program computers, that when we create predictive models we always call our input features “X” and our output features “Y”. So let’s just quickly do that, we have a DataFrame containing our data from which we want to extract **gpa** (our input feature) and **gre** (our ouput feature) and store them respectively in variables X and Y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjUYqF6NDcpa"
   },
   "outputs": [],
   "source": [
    "X = df[['gpa']]\n",
    "Y = df[['gre']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nilLnF7GDcpd"
   },
   "source": [
    "Note that this time we don’t just use data.gpa to extract the field’s data. This is because we want it in the form of a matrix (and not a vector, which is what data.gpa would give us). If you want to see the difference try following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SaCuSfVwDcpe"
   },
   "outputs": [],
   "source": [
    "print(df.gpa.shape)\n",
    "print(df[['gpa']].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkELvv5xDcpi"
   },
   "source": [
    "When you run this you will see that data.gpa only has one dimension (it is a vector), whereas our new version has two dimensions (albeit being a matrix with only one row in it!). The difference is subtle but just trust me for now - this means it will be in the perfect format ready to feed to sklearn as it uses matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjWelANFDcpk"
   },
   "source": [
    "### Step D4 - Linear Regression\n",
    "\n",
    "Create a linear regression model, not by dropping on a widget like Orange... but by typing the following couple of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_g8yarlHDcpl"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(X,Y)\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_FWCHoHDcpo"
   },
   "source": [
    "Et voila.  Job done. The first line creates an ‘empty’ model ready for us to train. The second line does the training by finding the correct line that ‘fits’ the data. The final line prints out the $m$ and the $c$ in the equation $y = mx+c$. Job done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YL06dQ7IDcpp"
   },
   "source": [
    "##### Quick challenge: \n",
    "\n",
    "Any thoughts on what the connection between the gradient coefficient (coef_) and correlation (r) are? DON’T CHEAT ;)\n",
    "##### Answer: \n",
    "\n",
    "If you standardize your data the correlation is the gradient coefficient. The closer that line is to 1 (or -1) the stronger the link!\n",
    "\n",
    "Just to prove the above (and to show you how to standardize your data - which is vital for distance measure based predictors) add the following lines before you created your linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqB1YFu8Dcpp"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)\n",
    "Y_scaled = preprocessing.scale(Y.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMmtIYYjDcps"
   },
   "source": [
    "Sklearn gives us a special set of functions to do this sort of thing. By default scale() will standardize your data (i.e. mean centre the data and divide through by its variance), making all data “measure-less”. This means we can use it more readily in anything that compares distances, and all the datapoints are now comparable on any feature…. whatever the scale it originally used. \n",
    "\n",
    "**Re-run your LinearRegression fitting using X_scaled and Y_scaled this time, and confirm that the correlation you calculated via numpy is the same as the linear regression gradient coefficient!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZeuCoJqDcpt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k69wgvMDUTBz"
   },
   "source": [
    "### Step D5 - Matplotlib & Linear Regression\n",
    "\n",
    "We can plot the regression line obtained on top of the data in matplotlib using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7Ml2s6HUTB1"
   },
   "outputs": [],
   "source": [
    "plt.plot(df.gpa, df.gre, '+')\n",
    "lrline = model.coef_[0][0] * df.gpa + model.intercept_[0]\n",
    "plt.plot(df.gpa, lrline)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB-XzigNUTB8"
   },
   "source": [
    "### Step D6 - Seaborn & Linear Regression\n",
    "\n",
    "Try to obtain a similar visualisation using Seaborn and its lmplot() function. You can refer to the lecture slides and the online documentation of Seaborn to know more about its usage. Its a whole lot easier than the matplotlib code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvrEn6XTUTB-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNLxZhkkUTCJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4fkX_EWUTCO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9C_zLF6UTCT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FBA Tutorial 09b - Python Practice (Notebook).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
